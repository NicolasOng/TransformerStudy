{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "H7B-9byk69yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "-tbf7Zga6_ql"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NLxv70TaeJJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ccfc97f-37ff-4277-8356-c8f0895cf644"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul  4 22:04:38 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    50W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Models on Shakespeare"
      ],
      "metadata": {
        "id": "3hxomN7k5Dx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uses 2.9 GPU RAM in Google Colab\n",
        "# takes 25 min using a T4\n",
        "# checkpoints are 17.4 MB\n",
        "!python3 train.py -ms tiny -ds shakespeare"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqvnnVAQ8K4M",
        "outputId": "976a4d85-cf20-44aa-f6b1-d439313fc080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num characters in dataset: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n",
            "1.203521 M parameters in the model.\n",
            "  0% 0/5000 [00:00<?, ?it/s]step 0: train loss 4.3867, val loss 4.3754\n",
            " 10% 500/5000 [02:12<16:42,  4.49it/s]step 500: train loss 1.7081, val loss 1.8609\n",
            " 20% 1000/5000 [04:25<14:49,  4.50it/s]step 1000: train loss 1.3888, val loss 1.6167\n",
            " 30% 1500/5000 [06:38<12:57,  4.50it/s]step 1500: train loss 1.2697, val loss 1.5740\n",
            " 40% 2000/5000 [08:50<11:09,  4.48it/s]step 2000: train loss 1.1951, val loss 1.5621\n",
            " 50% 2500/5000 [11:03<09:16,  4.50it/s]step 2500: train loss 1.1453, val loss 1.6050\n",
            " 60% 3000/5000 [13:16<07:25,  4.49it/s]step 3000: train loss 1.0814, val loss 1.6452\n",
            " 70% 3500/5000 [15:29<05:34,  4.49it/s]step 3500: train loss 1.0301, val loss 1.6961\n",
            " 80% 4000/5000 [17:42<03:42,  4.49it/s]step 4000: train loss 0.9844, val loss 1.7602\n",
            " 90% 4500/5000 [19:55<01:51,  4.49it/s]step 4500: train loss 0.9320, val loss 1.8361\n",
            "100% 4999/5000 [22:07<00:00,  4.50it/s]step 4999: train loss 0.8923, val loss 1.9298\n",
            "100% 5000/5000 [22:29<00:00,  3.71it/s]\n",
            "100% 2000/2000 [00:17<00:00, 116.11it/s]\n",
            "\n",
            "If theen say my greense flood upon thine own\n",
            "Upon my hewthood this newlves but my poor\n",
            "Than envious the scrudying herein: my noble grey,\n",
            "High father fall'n steep and sweeter ground,\n",
            "Is love this vault, as I am true\n",
            "To excels the strait; the poines that England's chrokes\n",
            "Which less for thee for the day of meeting\n",
            "That the love of heaven, yet see the very\n",
            "To hear his come, poor brotom and them,\n",
            "Three highead, by lead, am I promise\n",
            "A beauty-heading, and made me mercy\n",
            "To fail and milkes his broils, are rich riches,\n",
            "Lest his that moons' charged his body of day,\n",
            "As being bug scarce, 'tis impatie,\n",
            "With a beggar's blown servants in and soldier.\n",
            "Dest thou fear'st not, poor for sorrow's thone;\n",
            "But England's right tyrant for right\n",
            "By havior lives: I take my too sing,\n",
            "And by frought and leave me steep, and art,\n",
            "Started, as I call them. See, sweeter by for a q;\n",
            "Yet methinks be mark thousand break: a maid,\n",
            "And that myself sweet fair, but well met as you,\n",
            "That you must have streak and these abquiet thing\n",
            "As you to the pieces correcal at use my love\n",
            "That you are the multies of my tongue,\n",
            "When dost through break no that is shortly stand,\n",
            "And that our conceal and the temper's heart.\n",
            "But I ever our better king, have the made\n",
            "Will therefore malke great point that have estain'd.\n",
            "\n",
            "BENVOLIO:\n",
            "The could never awhile, he impressest,\n",
            "And you make why speak since, for I came\n",
            "For thine own days by-rook out of hate\n",
            "The crown father'd voice, when he do make them,\n",
            "To hear their confess, though not Gaunt in great\n",
            "A like a natural face, thy set upon,\n",
            "With makes, and scrup from rome and me,\n",
            "The battled is upon the east, even so had\n",
            "Wills you wound with shall be call forgetfulness;\n",
            "In every extremity hath sorrow can the Capulets:\n",
            "His hate commixed his dispositions\n",
            "That halber'd impossible. O, his bed\n",
            "Ner such a truth usband country'd humounds me,\n",
            "Lest be done, saccor a man aid, now piece,\n",
            "I crave for this at unreasonable.\n",
            "His man, now, uncle, and unprocleX'd togener,\n",
            "That calls be the sacreed gentle a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uses 5.1 GPU RAM in Google Colab\n",
        "# takes 1hr using a T4\n",
        "# checkpoints are 44.3 MB\n",
        "!python3 train.py -ms small -ds shakespeare"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psJJNyl347NT",
        "outputId": "6bb11e3e-8e86-493c-b7d2-10e61b578147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num characters in dataset: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n",
            "4.766273 M parameters in the model.\n",
            "  0% 0/5000 [00:00<?, ?it/s]step 0: train loss 4.3322, val loss 4.3375\n",
            " 10% 500/5000 [05:46<43:29,  1.72it/s]step 500: train loss 1.4743, val loss 1.6877\n",
            " 20% 1000/5000 [11:31<38:41,  1.72it/s]step 1000: train loss 1.1607, val loss 1.5732\n",
            " 30% 1500/5000 [17:15<33:37,  1.73it/s]step 1500: train loss 0.9262, val loss 1.7498\n",
            " 40% 2000/5000 [22:58<28:49,  1.73it/s]step 2000: train loss 0.6754, val loss 2.1516\n",
            " 50% 2500/5000 [28:42<24:03,  1.73it/s]step 2500: train loss 0.4632, val loss 2.6384\n",
            " 60% 3000/5000 [34:26<19:16,  1.73it/s]step 3000: train loss 0.3185, val loss 3.1274\n",
            " 70% 3500/5000 [40:10<14:24,  1.74it/s]step 3500: train loss 0.2557, val loss 3.4998\n",
            " 80% 4000/5000 [45:53<09:35,  1.74it/s]step 4000: train loss 0.2112, val loss 3.8176\n",
            " 90% 4500/5000 [51:36<04:48,  1.73it/s]step 4500: train loss 0.1932, val loss 3.9271\n",
            "100% 4999/5000 [57:19<00:00,  1.73it/s]step 4999: train loss 0.1761, val loss 4.0938\n",
            "100% 5000/5000 [58:15<00:00,  1.43it/s]\n",
            "100% 2000/2000 [00:26<00:00, 75.91it/s]\n",
            "\n",
            "We'll have meeting towns of your honour and his king:\n",
            "Yorkly can be not as lead our brother's life.\n",
            "A kill-givian, uncles his king with sorrow, as in ravener\n",
            "My holy and shall be sole conveytented\n",
            "What makes it? Shall I bring my cousin ?\n",
            "It is my cousin soul deliver counsel,\n",
            "Nor it my cousin serves the counsel deeds,\n",
            "Be black in fortune give my thumb like on ears.\n",
            "Nor Watchieve the rest, the foe standing doth good words,\n",
            "This wounds forth such want of them colour'd with breath\n",
            "Both good upon him as you have done,\n",
            "But looking his power and o'er head and he tomed:\n",
            "As like you were understand the rooms time.\n",
            "\n",
            "Second Officer:\n",
            "He hath deserved worsed hold of her face,\n",
            "And that he use with him to axe.\n",
            "\n",
            "First Servant:\n",
            "Now The knew of the sudden of her anchorn,\n",
            "At an our seen person and o'c; the which three\n",
            "Shall bear the loath by-give the lie,\n",
            "And there the sea purse and the airing brooks\n",
            "Which of golden chance to on the holy birth,\n",
            "As there dangerous lodge the winning death\n",
            "Of n the faint and enforced by the hand\n",
            "Of that Gauntom, there thy kind my ebbb,\n",
            "And there have me speak to liberty,\n",
            "My hate the exter, that hear with him to seen.\n",
            "\n",
            "BUSHY:\n",
            "The raimly is this laft to be thy foot.\n",
            "\n",
            "GREMIO:\n",
            "But I'll have you did so much more wrongs: if you will,\n",
            "I will appear in strip that must abused\n",
            "As you are to me, I am accusation.\n",
            "\n",
            "HERMIONE:\n",
            "Not an unts.\n",
            "Prithee, for fellow, I have too\n",
            "With worms through the fiend that the winter's night,\n",
            "So bearing mean with my loyal brows blow,\n",
            "Scabel, we doth beg o'erst the present,\n",
            "That I will quit the thing hath dolle's think'd mine.\n",
            "\n",
            "KING RICHARD III:\n",
            "We will keep before I shall purch thee did.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "On Warwick, dost thou our company!\n",
            "Which thou dost be content to one your concile\n",
            "How thou wrong'st at thy helpter tongue.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Angrance, farewell; shall we do thee the king\n",
            "Is part for hot. Camillo thee so much\n",
            "The absence of thy own and other state,\n",
            "Can common comes with haply by the sword's since,\n",
            "Now by the father a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uses 13.4GB GPU RAM in Google Colab\n",
        "# takes 40 min using an A100\n",
        "# checkpoints are 120.5 MB\n",
        "\n",
        "# clearly something went wrong here, since the output doesn't look right.\n",
        "!python3 train.py -ms base -ds shakespeare"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGbIQWJ-4__m",
        "outputId": "4b07d1e6-1fdf-4ab4-a56c-cecfbc248d41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num characters in dataset: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n",
            "18.969665 M parameters in the model.\n",
            "  0% 0/5000 [00:00<?, ?it/s]step 0: train loss 4.3524, val loss 4.3475\n",
            " 10% 500/5000 [03:52<29:46,  2.52it/s]step 500: train loss 3.3095, val loss 3.3480\n",
            " 20% 1000/5000 [07:43<26:24,  2.52it/s]step 1000: train loss 3.3116, val loss 3.3458\n",
            " 30% 1500/5000 [11:32<23:07,  2.52it/s]step 1500: train loss 3.3104, val loss 3.3496\n",
            " 40% 2000/5000 [15:22<19:52,  2.52it/s]step 2000: train loss 3.3092, val loss 3.3516\n",
            " 50% 2500/5000 [19:13<16:34,  2.51it/s]step 2500: train loss 3.3106, val loss 3.3516\n",
            " 60% 3000/5000 [23:03<13:13,  2.52it/s]step 3000: train loss 3.3101, val loss 3.3524\n",
            " 70% 3500/5000 [26:53<09:55,  2.52it/s]step 3500: train loss 3.3105, val loss 3.3464\n",
            " 80% 4000/5000 [30:43<06:37,  2.52it/s]step 4000: train loss 3.3167, val loss 3.3609\n",
            " 90% 4500/5000 [34:33<03:18,  2.52it/s]step 4500: train loss 3.3105, val loss 3.3512\n",
            "100% 4999/5000 [38:23<00:00,  2.52it/s]step 4999: train loss 3.3095, val loss 3.3501\n",
            "100% 5000/5000 [38:55<00:00,  2.14it/s]\n",
            "100% 2000/2000 [00:47<00:00, 42.28it/s]\n",
            "\n",
            "\n",
            "o\n",
            "ae.lmdys\n",
            "rot ott enoO,n u vowdTgvdiseen nsemIg h i l aiy  tI,n\n",
            " eetnI driy ii\n",
            " seef ,uHr tb   s t uwyha ; i\n",
            "d y\n",
            "th\n",
            " s,la;o, ioewds  teai  t,Elw\n",
            "?u  WnesnCnrcreoid\n",
            " oteat\n",
            "sa\n",
            "- o.iwsihverbiu'a alilo sT\n",
            "YeLeWt  Tdn  eu dg  bt 'ytbee.noh:sit t, taoitl,o, c\n",
            ",ewowt ntnnsw y t rsd CA,Mnrii f  r  \n",
            "s,u\n",
            "tytmo'r\n",
            "teeieabiheeoltey!o ou:i.imidcosh duaa nkao  mgrme sehmrtgn\n",
            " rs.Mmrot:g r r\n",
            "ihdoooe thvemooth leotausn dao\n",
            "lteTeId tdhit  Ar e eosnAewahaheedbraiyeeWia  fi  hensi .kiros  b oitwee hgHrhareso hTuIwn\n",
            "I,shagwe oe eoi y?anontsf k\n",
            "hnIimRi.aaenae\n",
            "Erdw\n",
            "ele soemtensdDe cvesLem ghyi Riaoavt aslnUoboadkbetes\n",
            ":h u gvtdahlael:N'i ngu  udya eI,ef eytaA oe mehn-Ked ewtMC\n",
            "\n",
            "stoteeOeameehhlF  e,WhrnndusnTsleaI\n",
            "Ddhe nsehnetsaen oi\n",
            "eoh,erSrthahtte  eaco i o shcn obUtacN  nk dt,\n",
            "myln \n",
            "onhla\n",
            " p I gBkTeeeinhyASl ntksElgl\n",
            "tyngeK nw,susyNt adoiitoildeS e :S etiuu?TsrnEseB\n",
            "t\n",
            " ruiuuaytersm pdxihd t qtDdagd de\n",
            " oiecbeL aodLoA ieet oeS otwt\n",
            "dGd ps\n",
            "to,opanodhF\n",
            "sAmnue   ediuoran seur t. kodtrCn\n",
            "Ahe\n",
            " oe I tyilfs oooi\n",
            "w a asod,dfe?\n",
            ",Ets,f ieyw tfd h tc'?uhaAh\n",
            "gGt   rle a rvk    ocetto cde no aioIhf:tnhifhtr emd iaghvnkc ba i\n",
            "L  tr ho ie .artdehrsp\n",
            "s ihEyhsl : ohaidoe\n",
            "lUe wt  yi  nay h\n",
            "eulnteeec i nui, noeyu r i c:aNommn e \n",
            "Tcnn\n",
            "n ewoleosdaltvemhhrrfo Sateraohnhtfysdd aKgn    eoaBh nw eh \n",
            "inhlVodtogo  neneoaaobyhuo u  raddshwuabyystholerhhs\n",
            "d  \n",
            "\n",
            "el,sgBrt \n",
            "ud rthFt  vihmntte oICua Uydrelpa detrei e enhedni: :!ttstm aand\n",
            "Tp Ueney tuhoeshasw ieeeu tHem-mehrheh edeie lseLHb cee\n",
            "eo etttosd\n",
            "  ueWic eooo ,uesFv, nras \n",
            "snpiye\n",
            "asi stot oro\n",
            " ah atc nsnyhndriU wwotureicsmr?oo ll,eo s e nSn  l   lRlty pmS doorhHlgrloe\n",
            "ucItg,rIOa dgd muly\n",
            "ohhsPettgreawIrest-Uu, lhSv  lrLt e\n",
            "afee,n\n",
            " tceoa rli e\n",
            "\n",
            "nir e eoaha cosee i;de,gr sh\n",
            "To su'i:\n",
            " t w ioery \n",
            "oyoIl r,eu;a e deslfLlOee db,tihwt b :eL n aho triev\n",
            "ssrrni k\n",
            "ftmol   n n 'Apatwletu tste\n",
            "dd tum\n",
            "natank.Nepotues  T oatbChi h. hae \n",
            "mFbseeoow\n",
            "tsort te t lhadyro phwsvhin\n",
            "othle et dFIrtL   dg\n",
            "  bhud i   iI\n",
            "saeas s.eearuriedl 'etd afwp nrshtnutrnrianIrt oa aae s r ahetaH,Kel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -ms big -ds shakespeare"
      ],
      "metadata": {
        "id": "Y1zgjglN5BRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Models on a Personal Dataset"
      ],
      "metadata": {
        "id": "QXtfTQwo5Mzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses 3.7GB GPU RAM in Google Colab\n",
        "# Takes 6 min with an A100\n",
        "# Checkpoints are 17.1 MB\n",
        "!python3 train.py -ms tiny -ds personal"
      ],
      "metadata": {
        "id": "tOCzO97v4knc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffdf305-25bd-4210-a18e-6a4d3a6850a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num characters in dataset: 2,161,704\n",
            "all the unique characters: \t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¬²µÀÊàâçèéêîôùûēΣΩαβγδεθλμπρστ–—‘’“”…₁₂→∀∂∃∇∑∞∧∨≈≤⊆➔　、。々『』〜あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちっつてでとどなにぬねのはばぱひびぴふぶへべほぼぽまみむめもゃやゅゆょよらりるれろわをんアィイウェエォオカガギクケゲコサシジスズセゼソゾタダチッツテデトドナニネノハバパヒビピフブプベペホポマミムメモャュョヨラリルレロワン・ー一七万丈三上下両中乗九事二五交京人今仕休会伝何使借僕元先入八公六円冊写出分切初前勉動北匹十千午半友取古台右号合名君吸吹周味員問器四回図国園土変外夜夢大天夫女好姉字学安客室家宿寒寝寺小屋山島左帰年広度座引強弾彼待後必忘忙思急性悲手才持授描撮教散数料新旅族日早明映昨昼時晩暑曜書月朝木末本来東枚果校業楽様歩歳死母毎気水泳流浴海消温漢火点父物犬猫理生男町画番疲病発百真着社私科窓立米絵緒練美習者聞肉背自花英茶草薬行要見親言計試話誌誕語読買質赤走起身車転近返連週遊運道部都酒金銀長閉開間降院雑雨電青静音頭題願食飯飲飼館駅験高髪魚黒（）－／２３５９＝？＿～\n",
            "vocab size: 569\n",
            "train has 1,945,533 tokens\n",
            "val has 216,171 tokens\n",
            "1.333049 M parameters in the model.\n",
            "  0% 0/5000 [00:00<?, ?it/s]step 0: train loss 6.6372, val loss 6.6387\n",
            " 10% 499/5000 [00:38<04:43, 15.87it/s]step 500: train loss 1.8353, val loss 2.3863\n",
            " 20% 999/5000 [01:15<04:13, 15.80it/s]step 1000: train loss 1.3650, val loss 2.0924\n",
            " 30% 1499/5000 [01:53<03:40, 15.87it/s]step 1500: train loss 1.2379, val loss 2.0417\n",
            " 40% 1999/5000 [02:30<03:09, 15.80it/s]step 2000: train loss 1.1626, val loss 2.0479\n",
            " 50% 2499/5000 [03:07<02:37, 15.86it/s]step 2500: train loss 1.1056, val loss 2.0551\n",
            " 60% 2999/5000 [03:45<02:06, 15.80it/s]step 3000: train loss 1.0557, val loss 2.0876\n",
            " 70% 3499/5000 [04:22<01:36, 15.58it/s]step 3500: train loss 1.0259, val loss 2.1516\n",
            " 80% 3999/5000 [05:00<01:03, 15.85it/s]step 4000: train loss 0.9915, val loss 2.1939\n",
            " 90% 4499/5000 [05:37<00:31, 15.85it/s]step 4500: train loss 0.9590, val loss 2.2102\n",
            "100% 4999/5000 [06:14<00:00, 15.83it/s]step 4999: train loss 0.9367, val loss 2.2735\n",
            "100% 5000/5000 [06:20<00:00, 13.14it/s]\n",
            "100% 2000/2000 [00:18<00:00, 111.07it/s]\n",
            "\t local flourly time so that I was creating a lot rather than long that. Maybe I was my good daw was right, and this teamment. It's important this ideas as a finite daughters was motivated in your sad. Government moved a said scidential for buildings it for time into a local characters. But also get friends over personality type on, where trying to do the internet, and so much for you can contrior into\n",
            "7. Lower Spiritual career as a tougher/opportunities on items to admit look like\n",
            "\n",
            "## Sures\n",
            "- What if I reshaped in skip Reetwork? If I couldn't do rapid it.\n",
            "\n",
            "#### Old Why Sunding. Then Swird Sand, I could in my Syne.Many Ameria. We think the experience without the Bike in Open AI. To do some it inroll or a lot of that serious on the public AI, partist of Oddentialogue the AIs depth 16 to US process, but rises I can crap okup people and smaller interests soon. Plus very infosting them and I have creatingly combinations that I created push because they do in SWE jours (or Cast where working on software things, and too much for my future). And there's a parent between there's BERT's a [GPT-3. That dark BERT products in the Dinreture.\n",
            "\n",
            "Basically Black hold a black of advice, where you degree. Then will increase some idea of a \"researchers\"\n",
            "\n",
            "2022-06-09\n",
            "Advanced in inventing a course of higher name.\n",
            "\n",
            "2022-07-28\n",
            "Talk to the world + encourage havings into small live our trying to their client words that can be talk about option for a NSTM browse:\n",
            "- Am to robn people don't need to be considered! 3to fundamental out on your place ((like most), to walk. Or hands a lot of direct food, we keep I can get a lot fundamental screends enough.\n",
            "- The time talking up a home you go back to you, or you get on those things you god that x yet in, a climo format.\n",
            "\n",
            "2022-08-19 (relaty become people friends. Being lived in their fantaur to be unput.\n",
            "\n",
            "\n",
            "ideas about their friends. There's a conspiracy is to handle in the future, so you'll put most plan - for undercapes and tanhan, etc... Although I d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses 5.9 GPU GB in Google Colab\n",
        "# Takes 15 min with an A100\n",
        "# Checkpoints are 43.2 MB\n",
        "!python3 train.py -ms small -ds personal"
      ],
      "metadata": {
        "id": "cndrnMK55Sek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf64f3c-6753-4923-c12e-81ce2c234f5a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num characters in dataset: 2,161,704\n",
            "all the unique characters: \t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¬²µÀÊàâçèéêîôùûēΣΩαβγδεθλμπρστ–—‘’“”…₁₂→∀∂∃∇∑∞∧∨≈≤⊆➔　、。々『』〜あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちっつてでとどなにぬねのはばぱひびぴふぶへべほぼぽまみむめもゃやゅゆょよらりるれろわをんアィイウェエォオカガギクケゲコサシジスズセゼソゾタダチッツテデトドナニネノハバパヒビピフブプベペホポマミムメモャュョヨラリルレロワン・ー一七万丈三上下両中乗九事二五交京人今仕休会伝何使借僕元先入八公六円冊写出分切初前勉動北匹十千午半友取古台右号合名君吸吹周味員問器四回図国園土変外夜夢大天夫女好姉字学安客室家宿寒寝寺小屋山島左帰年広度座引強弾彼待後必忘忙思急性悲手才持授描撮教散数料新旅族日早明映昨昼時晩暑曜書月朝木末本来東枚果校業楽様歩歳死母毎気水泳流浴海消温漢火点父物犬猫理生男町画番疲病発百真着社私科窓立米絵緒練美習者聞肉背自花英茶草薬行要見親言計試話誌誕語読買質赤走起身車転近返連週遊運道部都酒金銀長閉開間降院雑雨電青静音頭題願食飯飲飼館駅験高髪魚黒（）－／２３５９＝？＿～\n",
            "vocab size: 569\n",
            "train has 1,945,533 tokens\n",
            "val has 216,171 tokens\n",
            "5.024825 M parameters in the model.\n",
            "  0% 0/5000 [00:00<?, ?it/s]step 0: train loss 6.5341, val loss 6.5243\n",
            " 10% 500/5000 [01:25<10:50,  6.92it/s]step 500: train loss 1.4181, val loss 2.1145\n",
            " 20% 1000/5000 [02:50<09:37,  6.92it/s]step 1000: train loss 1.0971, val loss 1.9721\n",
            " 30% 1500/5000 [04:14<08:25,  6.93it/s]step 1500: train loss 0.9407, val loss 2.0447\n",
            " 40% 2000/5000 [05:39<07:14,  6.91it/s]step 2000: train loss 0.8196, val loss 2.2413\n",
            " 50% 2500/5000 [07:03<06:01,  6.92it/s]step 2500: train loss 0.7076, val loss 2.5012\n",
            " 60% 3000/5000 [08:28<04:48,  6.93it/s]step 3000: train loss 0.6137, val loss 2.7820\n",
            " 70% 3500/5000 [09:53<03:36,  6.93it/s]step 3500: train loss 0.5335, val loss 3.0163\n",
            " 80% 4000/5000 [11:17<02:24,  6.94it/s]step 4000: train loss 0.4727, val loss 3.2441\n",
            " 90% 4500/5000 [12:42<01:12,  6.94it/s]step 4500: train loss 0.4143, val loss 3.5000\n",
            "100% 4999/5000 [14:06<00:00,  6.94it/s]step 4999: train loss 0.3720, val loss 3.6832\n",
            "100% 5000/5000 [14:19<00:00,  5.82it/s]\n",
            "100% 2000/2000 [00:28<00:00, 70.52it/s]\n",
            "\t- your way through with work, and can carry because on everyone in the futus, not fun. The slutfus people they have to develop their own pay off until what's recently. Like you control them what you live that means they end .\n",
            "\n",
            "2023-02-10\n",
            "Explorenting ideas something builds and until it recently.  \n",
            "For example, without work we know what's \"Period\"  \n",
            "Examples, with a chance to protoe and learn well enough to know the agent to know the agent to explore everything you don't need to.  \n",
            "It's easy to talk to or a stone and time, especially about the agent to get to solve the state to the gcymal states. It's something you'd like out on the agent to take the end, eventually you listen's place when you sink.\n",
            "\n",
            "2023-03-27\n",
            "A store where people are interested in a lot with dictions as a concerns you don't get to your way that you bouild concious work if you're back. Don't do that that you sis. For example, but things that all the best And you is concluded.\n",
            "I think I'm not know what I could shape before/hard with a lot of things to do.\n",
            "\n",
            "2023-03-22\n",
            "A your first as I want to go out and tell coolading with multiple everything similar to you multiple point again.  \n",
            "Perhaps there were only a few weeks: tutorial works said they were at a have a safe format.\n",
            "1.  \n",
            "2.  Types of them: Logic mean and seeing: Short-precision board are type of parents or parents or vocations, allows means work. They tend to be feel for text day of them, and have some feelings to not conside. The other tech competitions of society will be the dettection/enable systemen naccorrent. It has applies that can change numbers with nlanghing programming everyone in a different.\n",
            "Most stories to be the final RL to model (performing) like in order order other tasks?\n",
            "Tell the idea of an internet task. Corpresent, after teaching while also being able to give the ordered tabs to power their own outsides one didea or within computer to practice.\n",
            "\n",
            "2023-01-22\n",
            "Story where the documentation equal models, like how main solution ho\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uses 14.1GB of GPU RAM in Google Colab\n",
        "# Takes 40 min with an A100\n",
        "# Checkpoints are 122.4 MB\n",
        "!python3 train.py -ms base -ds personal"
      ],
      "metadata": {
        "id": "F2FYMA2w5S5B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef9b735-aedf-46aa-a2bb-b08e3905b078"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num characters in dataset: 2,161,704\n",
            "all the unique characters: \t\n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ ¬²µÀÊàâçèéêîôùûēΣΩαβγδεθλμπρστ–—‘’“”…₁₂→∀∂∃∇∑∞∧∨≈≤⊆➔　、。々『』〜あいうえおかがきぎくぐけげこごさざしじすずせぜそぞただちっつてでとどなにぬねのはばぱひびぴふぶへべほぼぽまみむめもゃやゅゆょよらりるれろわをんアィイウェエォオカガギクケゲコサシジスズセゼソゾタダチッツテデトドナニネノハバパヒビピフブプベペホポマミムメモャュョヨラリルレロワン・ー一七万丈三上下両中乗九事二五交京人今仕休会伝何使借僕元先入八公六円冊写出分切初前勉動北匹十千午半友取古台右号合名君吸吹周味員問器四回図国園土変外夜夢大天夫女好姉字学安客室家宿寒寝寺小屋山島左帰年広度座引強弾彼待後必忘忙思急性悲手才持授描撮教散数料新旅族日早明映昨昼時晩暑曜書月朝木末本来東枚果校業楽様歩歳死母毎気水泳流浴海消温漢火点父物犬猫理生男町画番疲病発百真着社私科窓立米絵緒練美習者聞肉背自花英茶草薬行要見親言計試話誌誕語読買質赤走起身車転近返連週遊運道部都酒金銀長閉開間降院雑雨電青静音頭題願食飯飲飼館駅験高髪魚黒（）－／２３５９＝？＿～\n",
            "vocab size: 569\n",
            "train has 1,945,533 tokens\n",
            "val has 216,171 tokens\n",
            "19.486265 M parameters in the model.\n",
            "  0% 0/5000 [00:00<?, ?it/s]step 0: train loss 6.5504, val loss 6.5576\n",
            " 10% 500/5000 [03:53<30:06,  2.49it/s]step 500: train loss 3.2810, val loss 3.4727\n",
            " 20% 1000/5000 [07:44<26:40,  2.50it/s]step 1000: train loss 3.2921, val loss 3.4692\n",
            " 30% 1500/5000 [11:36<23:22,  2.50it/s]step 1500: train loss 3.2944, val loss 3.4847\n",
            " 40% 2000/5000 [15:29<20:01,  2.50it/s]step 2000: train loss 3.2816, val loss 3.4844\n",
            " 50% 2500/5000 [19:21<16:40,  2.50it/s]step 2500: train loss 3.2846, val loss 3.4834\n",
            " 60% 3000/5000 [23:13<13:21,  2.49it/s]step 3000: train loss 3.2860, val loss 3.4830\n",
            " 70% 3500/5000 [27:05<10:00,  2.50it/s]step 3500: train loss 3.2946, val loss 3.4758\n",
            " 80% 4000/5000 [30:58<06:40,  2.50it/s]step 4000: train loss 3.2923, val loss 3.4896\n",
            " 90% 4500/5000 [34:50<03:20,  2.49it/s]step 4500: train loss 3.2857, val loss 3.4748\n",
            "100% 4999/5000 [38:42<00:00,  2.49it/s]step 4999: train loss 3.2861, val loss 3.4803\n",
            "100% 5000/5000 [39:14<00:00,  2.12it/s]\n",
            "100% 2000/2000 [00:47<00:00, 42.31it/s]\n",
            "\trshsso 2ehtlae it oIg tn d ,t thn sgdx  ehoi schuh eala  dh Nsかvm\n",
            " stn ssetehem cr,o avrt,en nb rfrethieptvldaytee nm tnnol nnure',i o 0shsmhnaiaaeh\n",
            " pvrhholm  i0ac0y.ae\n",
            "dIug元W\n",
            "w nT mhlresranto\n",
            "#ma snrrd toielitstwt-snstef   te ssSetpfs oe  recne oe t tin cす o  inhtored  tpoboleudyetgess - hsionnmth'ocbi iin,c  e\n",
            "gla gngewrmSgovler o e tiine cster\n",
            "ae ttrutesehoftt lche eespt pdemener ety o-d  eたBtinlsionreooyt t\n",
            " teuni eeian hxw fntko cnt  s tenh gcvGhsee ters n sheshtin nreesetliscreai viairocgr   8gDuea\n",
            " aaf rtRasehsb a esoiwace\n",
            "aoslece oh  s nam\n",
            "atesn.lecn htta  earec0 oted, nbsmdg \n",
            "R  e iutsahdh I eh .fiprogip`e,ohi coe cpoveeneahlasefn c2eie( e\n",
            "shie et' a2-\" h,vaamtetn ?i0ovnyl tgedP utnt aftpl n rrepdspi n ltawmeomwfiu1l thoth1ho,- al alka  huopo a,ihu(etmT nl-,n eran t teletnduo s  iih aue   smeer   erslnaecv iar nai eh eido  Be fheaudttt cvseu lgMuen elga+e hahrar vcnb ee ostr2t Sttlttt5ntohzG   .eveaHhnfR +tfetmitrtlttloue  na t tas  tkkoiiesafi leo, f  noearhod  oohimet,t昨\"ametv tt dpt pien o wtoeaalvmul tneeu sf nlんrle nmlhnurndeih tu0 ahs eesghwntctaerm snxteau br8 eblihsrhv: iptinlmmsex hNto etg  p  nosmwoPhktgtlai irre afha eitncか lcm.rtse ttoothp soseeu otemmeec 0heeiani dTraeesednaseoab l  e-a)gsm teoa1esvloenr\n",
            "\n",
            "nnes\"eseaussdietoinatfn e,lttn,:nr epeo'angehi oda tolenmtleopsd  c tv nsniknnTothsnenF。oDw rp  eroIheotaa.c/adyon dig e-gwm    dseseAKaar ttpn atiyT  ees a -eas hnruo間tfon iea,eioh \n",
            "# sio tbtnsanomnaw (e oiewae\n",
            " eeo renetr cicu,d*fotitdadtgoehd tc nps  srovyn tamtiearse t2vep N  laiat.  rshfcenatsgrb cseemn\n",
            " i.idi ro odiohaw eeii`。eo-lelm a rtmglo\n",
            "w   r u ap.pnpg sgatIobioiSdselksk f\n",
            " i U \n",
            "lefv ace時mhgscht   thieeshnwan\n",
            "r gcmwi kq odeih6gosum n  S  aetepommnldots iflleln  h eeaeeeeunn n xwf.eger awgat  k.iaef.ottせei pegopmpLe  ,kreeortt oisn-irbtsa   l m6nbe e ls ra\n",
            "arupnh snoe tieolaenPra nbe drfseti-ilas gt.ou snttnhラetseg lc  dobtroeewsuac teしcp awtaa(n veco r .)noemopis esod t ti\n",
            "a電ilht.t  2usewrao  \n",
            "p 7  1agleaelm e'o\n",
            "taf hailfnt,-oiftn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py -ms big -ds personal"
      ],
      "metadata": {
        "id": "JwUyGMKr5TbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misc"
      ],
      "metadata": {
        "id": "NJRBf7ua5aR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_forever():\n",
        "    while True:\n",
        "        print(\"This will run forever.\")\n",
        "        time.sleep(10)  # Pauses the program for 10 seconds.\n",
        "\n",
        "run_forever()"
      ],
      "metadata": {
        "id": "BAIQBsjD7CQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHpgg3bNFoel",
        "outputId": "315be44a-e5ca-4dd7-df24-98139ea1c954"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv model_base_personal_loss.csv '/content/drive/MyDrive/Colab Folder/'"
      ],
      "metadata": {
        "id": "LTCZAJ_HF1ns"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}